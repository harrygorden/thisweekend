# üîß API Fixes - Anvil HTTP Implementation

## Issues Fixed

### Issue 1: HTTP Response Handling ‚úÖ FIXED
**Problem:** Anvil's `http.request()` works differently than standard Python `requests` library

**Error:**
```
'StreamingMedia' object has no attribute 'status_code'
```

**Root Cause:**
- Standard Python: `response = requests.get(url)` ‚Üí Returns object with `.status_code`, `.text`, etc.
- Anvil Python: `response = anvil.http.request(url)` ‚Üí Returns content directly (string), throws exception on error

**Fixed In:**
- `server_code/weather_service.py`
- `server_code/scraper_service.py`
- `server_code/ai_service.py`

**Before:**
```python
response = anvil.http.request(url, ...)
if response.status_code != 200:  # ‚ùå This doesn't exist!
    raise Exception(...)
weather_data = json.loads(response.get_text())  # ‚ùå This doesn't exist!
```

**After:**
```python
# Anvil returns content directly, throws exception on error
response_text = anvil.http.request(url, ...)
weather_data = json.loads(response_text)  # ‚úÖ Works!
```

### Issue 2: Datetime Comparison ‚úÖ FIXED
**Problem:** Comparing timezone-aware and timezone-naive datetimes

**Error:**
```
can't compare offset-naive and offset-aware datetimes
```

**Root Cause:**
- `datetime.now()` ‚Üí timezone-naive
- Anvil Data Tables datetime columns ‚Üí might be timezone-aware
- Python can't compare the two

**Fixed In:**
- `server_code/background_tasks.py` - `cleanup_old_data()` function

**Before:**
```python
if event["scraped_at"] < event_cutoff:  # ‚ùå Might fail!
    event.delete()
```

**After:**
```python
try:
    scraped_at = event["scraped_at"]
    if scraped_at:
        # Remove timezone info for comparison if present
        if hasattr(scraped_at, 'replace') and scraped_at.tzinfo is not None:
            scraped_at = scraped_at.replace(tzinfo=None)
        if scraped_at < event_cutoff:  # ‚úÖ Safe comparison!
            event.delete()
except (TypeError, AttributeError):
    pass  # Skip on error
```

## üöÄ How to Test

### Step 1: Push the Fixes

```bash
git add server_code/
git commit -m "Fix Anvil HTTP API handling and datetime comparisons"
git push origin main
```

### Step 2: Pull in Anvil

1. Open Anvil
2. Click "Pull from Git"
3. Wait for sync

### Step 3: Run Data Refresh

In AdminForm:

1. Click **"4. Refresh Data"**
2. Confirm the prompt
3. Watch **Server Logs**

### Step 4: Expected Output

You should now see successful progress:

```
üöÄ BACKGROUND TASK STARTED
Starting scheduled data refresh at 2025-10-30...
============================================================

[Step 1/10] Cleaning up old data...
  ‚úì Cleanup complete                           ‚Üê ‚úÖ Fixed!
[Step 2/10] Fetching weekend weather...
Fetching weekend weather from OpenWeather API...
  ‚úì Fetched weather for 3 days                 ‚Üê ‚úÖ Fixed!
[Step 3/10] Saving weather data...
  ‚úì Weather data saved
[Step 4/10] Scraping weekend events...
  ‚úì Events scraped                             ‚Üê ‚úÖ Fixed!
[Step 5/10] Parsing events...
  ‚úì Found 42 events
[Step 6/10] Saving events to database...
  ‚úì 42 events saved
[Step 7/10] Analyzing events with AI...        ‚Üê ‚úÖ Fixed!
  Analyzing event 1/42: Live Music at...
  ...
  ‚úì Analyzed 42 events
[Step 8/10] Updating events with analysis...
  ‚úì Updated 42 events
[Step 9/10] Matching events with weather...
  ‚úì Matched 42 events
[Step 10/10] Calculating recommendation scores...
  ‚úì Scores calculated

============================================================
‚úÖ Data refresh completed successfully!
Duration: 187.3 seconds
Events found: 42
Events analyzed: 42
============================================================
```

**Duration:** 2-5 minutes (depending on number of events)

## üìä What Each API Does

### OpenWeather API (Step 2)
- **Endpoint:** `api.openweathermap.org/data/3.0/onecall`
- **Method:** GET
- **Returns:** JSON with daily + hourly weather
- **Duration:** ~5-10 seconds
- **Result:** 3 rows in `weather_forecast` table

### Firecrawl API (Step 4)
- **Endpoint:** `api.firecrawl.dev/v1/scrape`
- **Method:** POST
- **Returns:** JSON with markdown content
- **Duration:** ~10-20 seconds
- **Result:** 20-60 events parsed

### OpenAI API (Step 7)
- **Endpoint:** `api.openai.com/v1/chat/completions`
- **Method:** POST
- **Returns:** JSON with AI analysis
- **Duration:** ~60-120 seconds (for 50 events)
- **Rate Limit:** 0.5 seconds between calls
- **Result:** Events categorized and tagged

## ‚úÖ Verification Checklist

After the task completes:

### Check Data Tables

**weather_forecast:**
- [ ] Has 3 rows (Friday, Saturday, Sunday)
- [ ] `temp_high` and `temp_low` are filled
- [ ] `conditions` has weather description
- [ ] `hourly_data` is populated

**events:**
- [ ] Has 20-60 rows (depending on website)
- [ ] `title` and `description` are filled
- [ ] `date` and `start_time` are set
- [ ] `is_indoor` and `is_outdoor` are set (from AI)
- [ ] `categories` list is populated (from AI)
- [ ] `weather_score` is calculated (0-100)
- [ ] `recommendation_score` is calculated (0-100)

**scrape_log:**
- [ ] Has 1 new row
- [ ] `status` = "success"
- [ ] `events_found` matches event count
- [ ] `events_analyzed` matches event count
- [ ] `duration_seconds` is 120-300

### Check AdminForm

Click **"View Refresh Log":**
- [ ] Shows last run time
- [ ] Shows "success" status
- [ ] Shows event counts
- [ ] Shows duration

Click **"Refresh Status":**
- [ ] Event count updates
- [ ] Weather count shows 3
- [ ] Last refresh time updates

## üêõ Troubleshooting

### If Weather Fetch Fails

**Check API Key:**
```python
# In server console:
import anvil.secrets
key = anvil.secrets.get_secret('OPENWEATHER_API_KEY')
print(f"Key: {key[:10]}...")  # First 10 chars
```

**Test Manually:**
```python
# In server console:
import server_code.weather_service as weather
data = weather.fetch_weekend_weather()
print(f"Days: {list(data.keys())}")
```

### If Scraping Fails

**Check Firecrawl Key:**
- In AdminForm, click "2. Test API Keys"
- Should show ‚úÖ for FIRECRAWL_API_KEY

**Test Manually:**
```python
# In server console:
import server_code.scraper_service as scraper
content = scraper.scrape_weekend_events()
print(f"Content length: {len(content)}")
```

### If AI Analysis Fails

**Common Issues:**
- Rate limiting (500ms delay between calls is built-in)
- Token limit exceeded (rare with our prompts)
- Invalid API key

**Test Manually:**
```python
# In server console:
import server_code.ai_service as ai
test_event = {
    'title': 'Test Concert',
    'description': 'Live music at the park',
    'location': 'Overton Park',
    'cost_raw': 'Free'
}
result = ai.analyze_event(test_event)
print(result)
```

## üìã Summary of Changes

**Files Modified:**
1. `server_code/weather_service.py` - HTTP response handling
2. `server_code/scraper_service.py` - HTTP response handling
3. `server_code/ai_service.py` - HTTP response handling
4. `server_code/background_tasks.py` - Datetime comparison fix

**Lines Changed:** ~30 lines across 4 files

**Impact:** 
- ‚úÖ Weather API now works
- ‚úÖ Scraping API now works
- ‚úÖ AI API now works
- ‚úÖ Cleanup doesn't error
- ‚úÖ Complete pipeline functional

## üéØ Expected Result

After pushing these fixes, your data refresh should:

1. ‚úÖ Complete without errors
2. ‚úÖ Populate weather_forecast (3 rows)
3. ‚úÖ Populate events (20-60 rows)
4. ‚úÖ Add AI analysis to all events
5. ‚úÖ Calculate weather scores
6. ‚úÖ Calculate recommendation scores
7. ‚úÖ Log success in scrape_log

**Total time:** 2-5 minutes

---

**Push to GitHub and test! The background task should now work end-to-end.** üöÄ

